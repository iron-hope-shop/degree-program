{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2.1 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2.1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.1\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Define constants for the CIFAR-10 dataset, model training parameters, and model architecture\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Print data shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"{X_train.shape[0]} train samples\")\n",
    "print(f\"{X_test.shape[0]} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Normalize data to range [0,1]\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               4194816   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4200842 (16.02 MB)\n",
      "Trainable params: 4200842 (16.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 10s 30ms/step - loss: 1.7915 - accuracy: 0.3702 - val_loss: 1.4734 - val_accuracy: 0.4659\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 10s 30ms/step - loss: 1.3955 - accuracy: 0.5060 - val_loss: 1.2978 - val_accuracy: 0.5361\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 10s 30ms/step - loss: 1.2524 - accuracy: 0.5549 - val_loss: 1.1979 - val_accuracy: 0.5863\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 9s 30ms/step - loss: 1.1644 - accuracy: 0.5909 - val_loss: 1.1075 - val_accuracy: 0.6125\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 10s 30ms/step - loss: 1.0885 - accuracy: 0.6177 - val_loss: 1.1787 - val_accuracy: 0.5875\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 1.0291 - accuracy: 0.6385 - val_loss: 1.0972 - val_accuracy: 0.6240\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.9787 - accuracy: 0.6586 - val_loss: 1.1532 - val_accuracy: 0.6064\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.9312 - accuracy: 0.6747 - val_loss: 1.0321 - val_accuracy: 0.6419\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.8925 - accuracy: 0.6884 - val_loss: 1.1256 - val_accuracy: 0.6171\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.8570 - accuracy: 0.6987 - val_loss: 1.0362 - val_accuracy: 0.6444\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.8228 - accuracy: 0.7115 - val_loss: 0.9899 - val_accuracy: 0.6640\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.7912 - accuracy: 0.7236 - val_loss: 1.0500 - val_accuracy: 0.6533\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.7567 - accuracy: 0.7344 - val_loss: 1.0594 - val_accuracy: 0.6489\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.7394 - accuracy: 0.7446 - val_loss: 0.9689 - val_accuracy: 0.6822\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.7114 - accuracy: 0.7528 - val_loss: 1.0828 - val_accuracy: 0.6592\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6892 - accuracy: 0.7606 - val_loss: 1.0307 - val_accuracy: 0.6681\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6680 - accuracy: 0.7703 - val_loss: 0.9742 - val_accuracy: 0.6804\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6500 - accuracy: 0.7746 - val_loss: 1.0302 - val_accuracy: 0.6681\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6258 - accuracy: 0.7820 - val_loss: 1.1527 - val_accuracy: 0.6499\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.6114 - accuracy: 0.7905 - val_loss: 1.0850 - val_accuracy: 0.6674\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 1.0748 - accuracy: 0.6630\n",
      "Test score: 1.0748430490493774\n",
      "Test accuracy: 0.6629999876022339\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIM,metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(f\"Test score: {score[0]}\")\n",
    "print(f\"Test accuracy: {score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture and weights for later use\n",
    "model_json = model.to_json()\n",
    "open(\"cifar10_architecture.json\", \"w\").write(model_json)\n",
    "model.save_weights(\"cifar10_weights.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a deeper model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 1.9732 - accuracy: 0.2673 - val_loss: 1.6589 - val_accuracy: 0.3830\n",
      "Epoch 2/40\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 1.6456 - accuracy: 0.3939 - val_loss: 1.5400 - val_accuracy: 0.4253\n",
      "Epoch 3/40\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 1.4762 - accuracy: 0.4635 - val_loss: 1.3338 - val_accuracy: 0.5258\n",
      "Epoch 4/40\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 1.3671 - accuracy: 0.5075 - val_loss: 1.3238 - val_accuracy: 0.5329\n",
      "Epoch 5/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.2917 - accuracy: 0.5370 - val_loss: 1.3113 - val_accuracy: 0.5441\n",
      "Epoch 6/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.2328 - accuracy: 0.5631 - val_loss: 1.0916 - val_accuracy: 0.6110\n",
      "Epoch 7/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.1717 - accuracy: 0.5859 - val_loss: 1.1660 - val_accuracy: 0.5904\n",
      "Epoch 8/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.1261 - accuracy: 0.6021 - val_loss: 1.0803 - val_accuracy: 0.6171\n",
      "Epoch 9/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.0955 - accuracy: 0.6162 - val_loss: 1.0587 - val_accuracy: 0.6198\n",
      "Epoch 10/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.0623 - accuracy: 0.6254 - val_loss: 1.0690 - val_accuracy: 0.6287\n",
      "Epoch 11/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.0259 - accuracy: 0.6403 - val_loss: 1.0227 - val_accuracy: 0.6376\n",
      "Epoch 12/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.0021 - accuracy: 0.6470 - val_loss: 0.9267 - val_accuracy: 0.6764\n",
      "Epoch 13/40\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.9793 - accuracy: 0.6560 - val_loss: 0.9391 - val_accuracy: 0.6693\n",
      "Epoch 14/40\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.9545 - accuracy: 0.6648 - val_loss: 0.9176 - val_accuracy: 0.6846\n",
      "Epoch 15/40\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.9328 - accuracy: 0.6708 - val_loss: 0.8970 - val_accuracy: 0.6868\n",
      "Epoch 16/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.9175 - accuracy: 0.6790 - val_loss: 0.9361 - val_accuracy: 0.6684\n",
      "Epoch 17/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.8996 - accuracy: 0.6823 - val_loss: 0.8671 - val_accuracy: 0.6963\n",
      "Epoch 18/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.8854 - accuracy: 0.6891 - val_loss: 0.8749 - val_accuracy: 0.6959\n",
      "Epoch 19/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.8777 - accuracy: 0.6938 - val_loss: 0.8759 - val_accuracy: 0.6913\n",
      "Epoch 20/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.8589 - accuracy: 0.6986 - val_loss: 0.9213 - val_accuracy: 0.6800\n",
      "Epoch 21/40\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.8457 - accuracy: 0.7030 - val_loss: 0.8663 - val_accuracy: 0.7011\n",
      "Epoch 22/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.8374 - accuracy: 0.7092 - val_loss: 0.8743 - val_accuracy: 0.6946\n",
      "Epoch 23/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.8265 - accuracy: 0.7106 - val_loss: 0.8342 - val_accuracy: 0.7088\n",
      "Epoch 24/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.8118 - accuracy: 0.7164 - val_loss: 0.8786 - val_accuracy: 0.7036\n",
      "Epoch 25/40\n",
      "313/313 [==============================] - 26s 82ms/step - loss: 0.8110 - accuracy: 0.7150 - val_loss: 0.8887 - val_accuracy: 0.7008\n",
      "Epoch 26/40\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.8077 - accuracy: 0.7160 - val_loss: 0.7906 - val_accuracy: 0.7241\n",
      "Epoch 27/40\n",
      "313/313 [==============================] - 25s 78ms/step - loss: 0.7865 - accuracy: 0.7236 - val_loss: 0.8435 - val_accuracy: 0.7027\n",
      "Epoch 28/40\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.7871 - accuracy: 0.7250 - val_loss: 0.8430 - val_accuracy: 0.7176\n",
      "Epoch 29/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7752 - accuracy: 0.7309 - val_loss: 0.8207 - val_accuracy: 0.7165\n",
      "Epoch 30/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7685 - accuracy: 0.7301 - val_loss: 0.8530 - val_accuracy: 0.7013\n",
      "Epoch 31/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7643 - accuracy: 0.7341 - val_loss: 0.8194 - val_accuracy: 0.7165\n",
      "Epoch 32/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7594 - accuracy: 0.7347 - val_loss: 0.7977 - val_accuracy: 0.7312\n",
      "Epoch 33/40\n",
      "313/313 [==============================] - 22s 72ms/step - loss: 0.7533 - accuracy: 0.7362 - val_loss: 0.8148 - val_accuracy: 0.7219\n",
      "Epoch 34/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7474 - accuracy: 0.7387 - val_loss: 0.8372 - val_accuracy: 0.7150\n",
      "Epoch 35/40\n",
      "313/313 [==============================] - 22s 72ms/step - loss: 0.7517 - accuracy: 0.7400 - val_loss: 0.8670 - val_accuracy: 0.7158\n",
      "Epoch 36/40\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.7434 - accuracy: 0.7393 - val_loss: 0.8156 - val_accuracy: 0.7275\n",
      "Epoch 37/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7437 - accuracy: 0.7413 - val_loss: 0.8015 - val_accuracy: 0.7248\n",
      "Epoch 38/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7382 - accuracy: 0.7446 - val_loss: 0.9051 - val_accuracy: 0.6988\n",
      "Epoch 39/40\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.7293 - accuracy: 0.7456 - val_loss: 0.8096 - val_accuracy: 0.7367\n",
      "Epoch 40/40\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.7318 - accuracy: 0.7444 - val_loss: 0.8443 - val_accuracy: 0.7208\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.8502 - accuracy: 0.7201\n",
      "Deeper Network Test score: 0.8502071499824524\n",
      "Deeper Network Test accuracy: 0.7200999855995178\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIM,metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=40, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(f\"Deeper Network Test score: {score[0]}\")\n",
    "print(f\"Deeper Network Test accuracy: {score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training set image...\n"
     ]
    }
   ],
   "source": [
    "# Augment training data using a generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "NUM_TO_AUGMENT = 5\n",
    "\n",
    "# Re-load CIFAR-10 data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(\"Augmenting training set image...\")\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "xtas, ytas = [], []\n",
    "for i in range(X_train.shape[0]):\n",
    "    num_aug = 0\n",
    "    x = X_train[i] # (3, 32, 32)\n",
    "    x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)\n",
    "    for x_aug in datagen.flow(x, batch_size=1, save_to_dir=\"./preview\", save_prefix=\"cifar\", save_format=\"jpeg\"):\n",
    "        if num_aug >= NUM_TO_AUGMENT:\n",
    "            break\n",
    "        xtas.append(x_aug[0])\n",
    "        num_aug += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pt/cz92vnj90z5gfg744c7r6pnc0000gn/T/ipykernel_21145/3605806118.py:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 58s 58s/step - loss: 2.3026 - accuracy: 0.1017\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 56s 56s/step - loss: 2.3026 - accuracy: 0.0987\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 57s 57s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 56s 56s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 55s 55s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 55s 55s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 56s 56s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 66s 66s/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# STEPS_PER_EPOCH = 50000\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# BATCH_SIZE = 50000\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# NB_EPOCH = 50\u001b[39;00m\n\u001b[1;32m      7\u001b[0m datagen\u001b[39m.\u001b[39mfit(X_train)\n\u001b[0;32m----> 9\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit_generator(\n\u001b[1;32m     10\u001b[0m     datagen\u001b[39m.\u001b[39;49mflow(X_train, Y_train, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE),\n\u001b[1;32m     11\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m] \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m BATCH_SIZE,\n\u001b[1;32m     12\u001b[0m     epochs\u001b[39m=\u001b[39;49mNB_EPOCH,\n\u001b[1;32m     13\u001b[0m     verbose\u001b[39m=\u001b[39;49mVERBOSE,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, Y_test, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, verbose\u001b[39m=\u001b[39mVERBOSE)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDeeper network with augmented data Test score: \u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/keras/src/engine/training.py:2810\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \n\u001b[1;32m   2800\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2801\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2805\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2806\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2807\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2808\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[0;32m-> 2810\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   2811\u001b[0m     generator,\n\u001b[1;32m   2812\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   2813\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m   2814\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2815\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   2816\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m   2817\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   2818\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m   2819\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   2820\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2821\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2822\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2823\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   2824\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m   2825\u001b[0m )\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexperimental_get_tracing_count\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    816\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns the number of times the function has been traced.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \n\u001b[1;32m    818\u001b[0m \u001b[39m  For more information on when a function is traced and when it is\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39m  traced multiple times see https://www.tensorflow.org/guide/function.\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[39m  Example:\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \n\u001b[1;32m    822\u001b[0m \u001b[39m  >>> @tf.function\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39m  ... def double(a):\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[39m  ...   return a + a\u001b[39;00m\n\u001b[0;32m--> 825\u001b[0m \u001b[39m  >>> double(tf.constant(1))\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39m  >>> double(tf.constant(2))\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[39m  >>> double.experimental_get_tracing_count()\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[39m  1\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[39m  >>> double(tf.constant(\"a\"))\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m  >>> double.experimental_get_tracing_count()\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m  2\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \n\u001b[1;32m    833\u001b[0m \n\u001b[1;32m    834\u001b[0m \u001b[39m  The first time experimental_get_tracing_count is called\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[39m  it returns 1, as the function is traced the first\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[39m  time it is called, and the second time the same graph is used\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[39m  since we're calling it with a parameter of the same type.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \n\u001b[1;32m    839\u001b[0m \u001b[39m  The second time experimental_get_tracing_count is called\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[39m  it returns 2, as we called double with a\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[39m  different argument type, and so it was traced again.\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \n\u001b[1;32m    843\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    844\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn\u001b[39m.\u001b[39mtracing_count \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    845\u001b[0m   result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39mtracing_count \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_functions_eagerly:\n\u001b[1;32m    856\u001b[0m   \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, tf_function_call\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 857\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    859\u001b[0m \u001b[39m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m# place.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minput_signature\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 148\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns the input signature.\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39minput_signature\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m attrs \u001b[39mand\u001b[39;00m IMPLEMENTS_ATTRIBUTE_NAME \u001b[39min\u001b[39;00m attrs:\n\u001b[1;32m   1326\u001b[0m   \u001b[39m# The alternative is to silently drop \"implements\" tag\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m   \u001b[39m# but it seems likely it would lead to hard to catch bugs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[39m# Anytime we annotate existing function we probably want to wrap\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m   \u001b[39m# it with safe read_value for backward compatibility.\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m   has_resource_vars \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[1;32m   1335\u001b[0m       inp\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mresource \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs)\n\u001b[1;32m   1337\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m((has_resource_vars, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_captured_inputs)), (\n\u001b[1;32m   1338\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mFunction \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m has \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{attr}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{value}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m attribute and thus can not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1339\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mdepend on any tensors outside of its signature or modify variables. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1340\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mNote: variables are always captured and cause function \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1341\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mre-tracing for every variable called.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1342\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m  inputs: \u001b[39m\u001b[39m{inputs}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  captures: \u001b[39m\u001b[39m{captured}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1343\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mTo pass a variable to such function use  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1344\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39muse variable.read_value().\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1345\u001b[0m           name\u001b[39m=\u001b[39mfunc_graph\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1346\u001b[0m           attr\u001b[39m=\u001b[39mIMPLEMENTS_ATTRIBUTE_NAME,\n\u001b[1;32m   1347\u001b[0m           value\u001b[39m=\u001b[39mattrs[IMPLEMENTS_ATTRIBUTE_NAME],\n\u001b[1;32m   1348\u001b[0m           inputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs,\n\u001b[0;32m-> 1349\u001b[0m           captured\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_captured_inputs))\n\u001b[1;32m   1350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_shapes \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m   1351\u001b[0m     output\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph\u001b[39m.\u001b[39moutputs)\n\u001b[1;32m   1352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m _parse_func_attrs(attrs \u001b[39mor\u001b[39;00m {})\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1453\u001b[0m   \u001b[39m# Reinitialize the physical device list after registering\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m   \u001b[39m# the pluggable device.\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_physical_devices(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlist_physical_devices\u001b[39m(\u001b[39mself\u001b[39m, device_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1458\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"List local devices visible to the system.\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \n\u001b[1;32m   1460\u001b[0m \u001b[39m  This API allows a client to query the devices before they have been\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[39m    List of PhysicalDevice objects.\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_physical_devices()\n",
      "File \u001b[0;32m~/apps/school/.conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m     52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m---> 53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare data augmentation configuration\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    epochs=NB_EPOCH,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(f\"Deeper network with augmented data Test score: {score[0]}\")\n",
    "print(f\"Deeper network with augmented data Test accuracy: {score[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical and Privacy Implications of Image Recognition Algorithms\n",
    "Image recognition algorithms have tremendous potential in various fields, including security, healthcare, and transportation. However, their usage is also associated with potential ethical and privacy implications.\n",
    "\n",
    "Privacy Concerns: One of the main concerns with image recognition algorithms, especially those capable of facial recognition, is the risk to individual privacy. If such systems are widely deployed, people's movements and activities could potentially be tracked and recorded without their knowledge or consent, leading to a significant loss of privacy (Galbusera et al. 2019).\n",
    "\n",
    "Bias and Discrimination: Machine learning algorithms, including those used for image recognition, can be biased based on the data they were trained on. For example, if an algorithm was mostly trained on images of people with a particular skin color, it might perform poorly when identifying individuals of a different skin color. This could lead to unintentional discrimination (Chen et al. 2018).\n",
    "\n",
    "Ethical Use: The use of image recognition technology in certain contexts, such as surveillance by governments or corporations, can raise ethical questions. Without clear regulations and transparency about when and how these systems are used, they can be misused, infringing upon civil liberties (Gichoya et al. 2021).\n",
    "\n",
    "To address these issues, it's crucial to establish clear ethical guidelines for the use of image recognition technology, as well as robust mechanisms for ensuring transparency and accountability in their deployment and operation. Furthermore, steps should be taken to ensure that the data used to train these systems is representative of the diverse range of individuals and environments they will be applied to, in order to minimize bias.\n",
    "\n",
    "References\n",
    "Chen, J., Konrad, J., & Ishwar, P. (2018). VGAN-based image representation learning for privacy-preserving facial expression recognition. *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*. https://doi.org/10.1109/cvprw.2018.00207 \n",
    "\n",
    "Galbusera, F., Casaroli, G., & Bassani, T. (2019). Artificial Intelligence and machine learning in Spine Research. *JOR SPINE, 2*(1). https://doi.org/10.1002/jsp2.1044 \n",
    "\n",
    "Wawira Gichoya, J., McCoy, L. G., Celi, L. A., &amp; Ghassemi, M. (2021). Equity in essence: A call for operationalising fairness in machine learning for Healthcare. *BMJ Health & Care Informatics, 28*(1). https://doi.org/10.1136/bmjhci-2020-100289 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
