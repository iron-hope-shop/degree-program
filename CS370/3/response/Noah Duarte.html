<!--
    Template for Blackboard discussion board replies.
    Created/Maintained by: Bradley Jackson <me@brad-jackson.com>
-->

<head>
    <p style="text-align: left; font-weight: bold;">
        Noah,
        <!-- UPDATE WITH OP'S NAME -->
        <span style="float: right">
            07/16/2023
        <!-- UPDATE -->
        </span>
    </p>
</head>

<body style="text-align: justify; padding: 8;">
    <br>
    <p style="line-height: 2; text-indent: 48px;">
        Noah, your post is really insightful and you've done a great job of highlighting the issues of hidden bias in AI, particularly in facial recognition technology and recommendation systems. Your emphasis on transparency, accountability, and the use of diverse datasets is spot on. However, I'd like to add a few thoughts to the mix. One approach that I think could be really beneficial is algorithmic auditing. This is a way of systematically checking how an AI system is performing, and it can cover a whole range of factors, including how fair it is and whether it's showing any signs of bias (Raji et al., 2020). By using this kind of auditing, we can spot if, say, a facial recognition system is struggling to accurately identify certain demographic groups. Once we know there's a problem, we can then take steps to fix it, whether that's by gathering more diverse training data or tweaking the algorithm itself.
        <!-- COPY/PASTA AS NEEDED FOR EACH SOURCE YOU HAVE! -->
    </p>
    <br>
    <p style="line-height: 2; text-indent: 48px;">
        Another thing that could be really useful is the use of fairness metrics during the development of these AI systems. These metrics give us a way to measure bias in a quantifiable way, which can help guide developers in creating algorithms that are more equitable and less likely to favor one group over another (Verma & Rubin, 2018).
        <!-- COPY/PASTA AS NEEDED FOR EACH SOURCE YOU HAVE! -->
    </p>
    <br>
    <p style="line-height: 2; text-indent: 48px;">
        When it comes to social media platforms, I think we could do a lot more to educate users about the potential for bias in the content they're seeing. If we can help people understand how these biases can create filter bubbles and echo chambers, and give them the tools to recognize and navigate these, we can empower individuals to seek out a broader range of perspectives.
        <!-- COPY/PASTA AS NEEDED FOR EACH SOURCE YOU HAVE! -->
    </p>
    <br>
    <p style="text-align: left; font-weight: bold;">
        Regards,
        <!-- UPDATE -->
    </p>
    <p style="text-indent: 48px; font-weight: bold;">
        Brad
        <!-- UPDATE -->
    </p>
    <h4 style="text-align: center !important;">
        References
        <!-- UPDATE IF NEEDED -->
    </h4>
    <p style="line-height: 2; padding-left: 48px; text-indent: -48px; line-height: 2;">
        Raji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI accountability gap. <i>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</i>. <a href="https://doi.org/10.1145/3351095.3372873">https://doi.org/10.1145/3351095.3372873</a>
        <!-- 
            REMEBER TO UPDATE REFERENCES TO APA 6 FORMAT!!
            USING ITALICS, QUOTES, ETC!
            COPY/PASTA AS NEEDED FOR EACH SOURCE YOU HAVE!
        -->
    </p>
    <p style="line-height: 2; padding-left: 48px; text-indent: -48px; line-height: 2;">
        Verma, S., & Rubin, J. (2018). Fairness definitions explained. <i>Proceedings of the International Workshop on Software Fairness</i>. <a href="https://doi.org/10.1145/3194770.3194776">https://doi.org/10.1145/3194770.3194776</a>
        <!-- 
            REMEBER TO UPDATE REFERENCES TO APA 6 FORMAT!!
            USING ITALICS, QUOTES, ETC!
            COPY/PASTA AS NEEDED FOR EACH SOURCE YOU HAVE!
        -->
    </p>
</body>